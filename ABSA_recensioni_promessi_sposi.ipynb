{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTF5lkVyYeuv"
      },
      "outputs": [],
      "source": [
        "!pip install jsonlines\n",
        "!pip install matplotlib\n",
        "!pip install adjustText"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cella per caricare i file di training e test di ABSITA 2020**"
      ],
      "metadata": {
        "id": "jgy8AqQDIK0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "#caricamento del file di absita\n",
        "uploaded = files.upload()\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "B4nCtXuoY3Ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Caricamento del modello e del tokenizer associato**"
      ],
      "metadata": {
        "id": "wCllhYv-I76T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jsonlines\n",
        "\n",
        "from transformers import AutoTokenizer, BertForTokenClassification, AdamW\n",
        "\n",
        "#caricamento del modello pre-addestrato\n",
        "\n",
        "#inserire il proprio token HF\n",
        "token=\"\"\n",
        "\n",
        "model_id = \"dbmdz/bert-base-italian-cased\"  #modello BERT italiano\n",
        "\n",
        "model = BertForTokenClassification.from_pretrained(model_id, num_labels=7, token=token) #aggiunta testa classificazione\n",
        "\n",
        "\n",
        "#caricamento tokenizer associato al modello BERT\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, token=token)\n"
      ],
      "metadata": {
        "id": "pQ5TIpc2Y40I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definizione di funzioni per la tokenizzazione e mappatura da caratteri a token**"
      ],
      "metadata": {
        "id": "W_FCwgWNJH5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#funzione per tokenizzare frasi utilizzando il tokenizer associato al modello BERT\n",
        "def tokenize_sentence(sentence):\n",
        "    return tokenizer(sentence, truncation=True, padding=\"max_length\", max_length=64)\n",
        "\n",
        "#funzione per mappare le posizioni dei caratteri nei token corrispondenti\n",
        "def map_char_positions_to_token_positions(sentence, char_positions, tokenizer):\n",
        "\n",
        "    tokens = tokenizer.tokenize(sentence)\n",
        "    token_offsets = tokenizer(sentence, return_offsets_mapping=True)[\"offset_mapping\"]\n",
        "\n",
        "    #mappa le posizioni (start, end) da caratteri a token\n",
        "    token_positions = []\n",
        "    for start_char, end_char in char_positions:\n",
        "        start_token = None\n",
        "        end_token = None\n",
        "        for idx, (start_offset, end_offset) in enumerate(token_offsets):\n",
        "            if start_token is None and start_offset >= start_char:\n",
        "                start_token = idx\n",
        "            if end_offset >= end_char:\n",
        "                end_token = idx\n",
        "                break\n",
        "        if start_token is not None and end_token is not None:\n",
        "            token_positions.append((start_token, end_token))\n",
        "\n",
        "    return token_positions"
      ],
      "metadata": {
        "id": "jvtg89otJG-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preparazione dei dati di training e test**"
      ],
      "metadata": {
        "id": "Ot0A9AoLJcaB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#apertura del file ndjson per il training\n",
        "with jsonlines.open(\"ate_absita_training_anon.ndjson\") as reader:\n",
        "    train_data = [obj for obj in reader]\n",
        "\n",
        "for entry in train_data:\n",
        "    sentence = entry[\"sentence\"]\n",
        "    char_positions = entry[\"aspects_position\"]\n",
        "\n",
        "    #mappa le posizioni dei caratteri a token\n",
        "    token_positions = map_char_positions_to_token_positions(sentence, char_positions, tokenizer)\n",
        "\n",
        "    #aggiorna il dizionario con le nuove posizioni\n",
        "    entry[\"token_positions\"] = token_positions\n",
        "\n",
        "\n",
        "#apertura del file ndjson per il test\n",
        "with jsonlines.open(\"ate_absita_dev.ndjson\") as reader:\n",
        "    test_data = [obj for obj in reader]\n",
        "\n",
        "for entry in test_data:\n",
        "    sentence = entry[\"sentence\"]\n",
        "    char_positions = entry[\"aspects_position\"]\n",
        "\n",
        "    #mappa le posizioni dei caratteri a token\n",
        "    token_positions = map_char_positions_to_token_positions(sentence, char_positions, tokenizer)\n",
        "\n",
        "    #aggiorna il dizionario con le nuove posizioni\n",
        "    entry[\"token_positions\"] = token_positions\n",
        "\n",
        "#definizione delle map per le etichette BIO e sentiment\n",
        "label_map = {(1, 0): 0, (0, 1): 1, (0, 0): 2, (1, 1): 3}\n",
        "bio_label_map = {\"B-Aspect\": 0, \"I-Aspect\": 1, \"O\": 2}\n",
        "\n",
        "#funzione che crea le etichette\n",
        "def create_labels(tokenized, entry):\n",
        "    bio_labels = [\"O\"] * len(tokenized[\"input_ids\"])  #inizializza BIO labels con \"O\"\n",
        "    sentiment_labels = [2] * len(tokenized[\"input_ids\"])  #inizializza sentiment labels con 2\n",
        "\n",
        "    if entry[\"token_positions\"] and entry[\"polarities\"]:\n",
        "       for (start, end), polarity in zip(entry[\"token_positions\"], entry[\"polarities\"]):\n",
        "          polarity_label = label_map.get(tuple(polarity), 6)  #converte la polaritÃ  in numero\n",
        "\n",
        "          for i in range(len(tokenized[\"input_ids\"])):\n",
        "              if i == start:\n",
        "                  bio_labels[i] = \"B-Aspect\"\n",
        "                  sentiment_labels[i] = polarity_label\n",
        "              elif i > start and i < end:\n",
        "                  bio_labels[i] = \"I-Aspect\"\n",
        "                  sentiment_labels[i] = polarity_label\n",
        "\n",
        "\n",
        "    bio_labels = [bio_label_map[label] for label in bio_labels]\n",
        "\n",
        "    return bio_labels, sentiment_labels\n",
        "\n",
        "train_data_tokenized = []\n",
        "test_data_tokenized = []\n",
        "\n",
        "#preparazione dei dati di training e test e creazione dei nuovi dizionari\n",
        "for entry in train_data:\n",
        "    tokenized = tokenize_sentence(entry[\"sentence\"])\n",
        "    bio_labels, sentiment_labels = create_labels(tokenized, entry)\n",
        "    train_data_tokenized.append({\n",
        "        \"input_ids\": tokenized[\"input_ids\"],\n",
        "        \"attention_mask\": tokenized[\"attention_mask\"],\n",
        "        \"bio_labels\": bio_labels,\n",
        "        \"sentiment_labels\": sentiment_labels\n",
        "    })\n",
        "\n",
        "for entry in test_data:\n",
        "    tokenized = tokenize_sentence(entry[\"sentence\"])\n",
        "    bio_labels, sentiment_labels = create_labels(tokenized, entry)\n",
        "    test_data_tokenized.append({\n",
        "        \"input_ids\": tokenized[\"input_ids\"],\n",
        "        \"attention_mask\": tokenized[\"attention_mask\"],\n",
        "        \"bio_labels\": bio_labels,\n",
        "        \"sentiment_labels\": sentiment_labels\n",
        "    })"
      ],
      "metadata": {
        "id": "CV0JM0nqY8p8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "#preparazione degli input e degli output\n",
        "\n",
        "train_input_ids = torch.tensor([item[\"input_ids\"] for item in train_data_tokenized])\n",
        "train_attention_masks = torch.tensor([item[\"attention_mask\"] for item in train_data_tokenized])\n",
        "train_bio_labels = torch.tensor([item[\"bio_labels\"] for item in train_data_tokenized])\n",
        "train_sentiment_labels = torch.tensor([item[\"sentiment_labels\"] for item in train_data_tokenized])\n",
        "\n",
        "test_input_ids = torch.tensor([item[\"input_ids\"] for item in test_data_tokenized])\n",
        "test_attention_masks = torch.tensor([item[\"attention_mask\"] for item in test_data_tokenized])\n",
        "test_bio_labels = torch.tensor([item[\"bio_labels\"] for item in test_data_tokenized])\n",
        "test_sentiment_labels = torch.tensor([item[\"sentiment_labels\"] for item in test_data_tokenized])\n",
        "\n",
        "#creazione del dataset\n",
        "train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_bio_labels, train_sentiment_labels)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size = 8, shuffle=True)\n",
        "train_len = len(train_dataloader)\n",
        "\n",
        "test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_bio_labels, test_sentiment_labels)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size = 8, shuffle=True)\n",
        "test_len = len(test_dataloader)"
      ],
      "metadata": {
        "id": "a4CGrK5uZAsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training**"
      ],
      "metadata": {
        "id": "LA6NlwUOLrCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from tqdm import tqdm\n",
        "from torch.nn import CrossEntropyLoss\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#permette di utilizzare gpu e cpu spostando il modello\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "#definizione dei pesi delle classi e funzioni di perdita\n",
        "class_weights = torch.tensor([1.0, 7.0, 0.1, 20.0]).to(device)\n",
        "bio_loss_fn = CrossEntropyLoss()\n",
        "sentiment_loss_fn = CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "#definizione dell'ottimizzatore per allenamento modello che definisce regola di aggiornamento dei pesi e gli iperparametri di training\n",
        "optimizer = AdamW(model.parameters(), lr=3e-5, weight_decay=1e-8) #weight_decay evita overfitting, lr Ã¨ tasso di apprendimento\n",
        "\n",
        "\n",
        "#parametri addestramento\n",
        "num_epochs = 100\n",
        "total_loss = 0.0\n",
        "prec_loss = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "    model.train() #modello in modalitÃ  training\n",
        "    train_loop = tqdm(train_dataloader, desc=\"Training\") #divisione dati batch\n",
        "\n",
        "    train_bio_loss = 0.0\n",
        "    train_sentiment_loss = 0.0\n",
        "\n",
        "    for i, batch in enumerate(train_loop):\n",
        "        #ricavo input e label del batch corrente\n",
        "        input_ids_batch, attention_masks_batch, bio_labels_batch, sentiment_labels_batch = batch\n",
        "        input_ids_batch, attention_masks_batch = input_ids_batch.to(device), attention_masks_batch.to(device)\n",
        "        bio_labels_batch, sentiment_labels_batch = bio_labels_batch.to(device), sentiment_labels_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()  #reset dei gradienti\n",
        "\n",
        "        #calcola output del modello su base degli input\n",
        "        outputs = model(input_ids_batch, attention_mask=attention_masks_batch)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        #divisione tra logits per BIO e sentiment, estrazione dei risultati con le 7 etichette\n",
        "        bio_logits = logits[..., :3]\n",
        "        sentiment_logits = logits[..., 3:]\n",
        "\n",
        "        #calcolo della loss per BIO e sentiment\n",
        "        bio_loss = bio_loss_fn(bio_logits.view(-1, 3), bio_labels_batch.view(-1))\n",
        "        sentiment_loss = sentiment_loss_fn(sentiment_logits.view(-1, 4), sentiment_labels_batch.view(-1))\n",
        "\n",
        "        #somma delle loss per eseguire una sola retropropagazione\n",
        "        total_loss = bio_loss + sentiment_loss\n",
        "        total_loss.backward()\n",
        "\n",
        "        #aggiorna i pesi sulla base dei gradienti calcolati\n",
        "        optimizer.step()\n",
        "\n",
        "        #aggiornamento della perdita totale per l'epoca\n",
        "        train_bio_loss += bio_loss.item()\n",
        "        train_sentiment_loss += sentiment_loss.item()\n",
        "\n",
        "        #aggiunta al progresso dell'epoca\n",
        "        train_loop.set_postfix(bio_loss=bio_loss.item(), sentiment_loss=sentiment_loss.item())\n",
        "\n",
        "    total_loss /= train_len\n",
        "\n",
        "    #condizione di arresto se la perdita non cambia sostanzialmente\n",
        "    if prec_loss != 0.0 and abs(prec_loss - total_loss) / prec_loss < 0.1:\n",
        "        break\n",
        "\n",
        "    #salvataggio delle perdite precedenti\n",
        "    prec_loss = total_loss\n",
        "\n",
        "    #stampa delle perdite dell'epoca\n",
        "    print(f\"Epoch {epoch + 1} - BIO Loss: {train_bio_loss / train_len:.4f}, Sentiment Loss: {train_sentiment_loss / train_len:.4f}\")"
      ],
      "metadata": {
        "id": "U1KpE34fZIDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Valutazione sui dati di training e test**"
      ],
      "metadata": {
        "id": "aSlr3gIXL_Sm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()  #modello in modalitÃ  di valutazione\n",
        "\n",
        "all_bio_predictions = []\n",
        "all_bio_labels = []\n",
        "all_sentiment_predictions = []\n",
        "all_sentiment_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in train_dataloader:  #dataloader del set di training\n",
        "        input_ids_batch, attention_masks_batch, bio_labels_batch, sentiment_labels_batch = batch\n",
        "\n",
        "        #mette i tensori sullo stesso dispositivo del model per evitare errori con gpu\n",
        "        input_ids_batch = input_ids_batch.to(device)\n",
        "        attention_masks_batch = attention_masks_batch.to(device)\n",
        "        bio_labels_batch = bio_labels_batch.to(device)\n",
        "        sentiment_labels_batch = sentiment_labels_batch.to(device)\n",
        "\n",
        "        outputs = model(input_ids_batch, attention_mask=attention_masks_batch)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        bio_logits = logits[..., :3]\n",
        "        sentiment_logits = logits[..., 3:]\n",
        "\n",
        "        bio_predictions = torch.argmax(bio_logits, dim=-1)\n",
        "        sentiment_predictions = torch.argmax(sentiment_logits, dim=-1)\n",
        "\n",
        "        all_bio_predictions.extend(bio_predictions.view(-1).cpu().numpy())\n",
        "        all_bio_labels.extend(bio_labels_batch.view(-1).cpu().numpy())\n",
        "        all_sentiment_predictions.extend(sentiment_predictions.view(-1).cpu().numpy())\n",
        "        all_sentiment_labels.extend(sentiment_labels_batch.view(-1).cpu().numpy())\n",
        "\n",
        "#calcolo metriche di valutazione per BIO e sentiment con classification_report\n",
        "bio_classification_report = classification_report(\n",
        "    all_bio_labels, all_bio_predictions, labels=list(bio_label_map.values()), target_names=list(bio_label_map.keys())\n",
        ")\n",
        "sentiment_classification_report = classification_report(\n",
        "    all_sentiment_labels, all_sentiment_predictions, labels=[0, 1, 2, 3], target_names=[\"positive\", \"negative\", \"neutral\", \"mixed\"]\n",
        ")\n",
        "\n",
        "print(\"BIO Classification Report:\")\n",
        "print(bio_classification_report)\n",
        "\n",
        "print(\"\\nSentiment Classification Report:\")\n",
        "print(sentiment_classification_report)"
      ],
      "metadata": {
        "id": "sjs6JUPQZKtt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()  #modello in modalitÃ  di valutazione\n",
        "\n",
        "all_bio_predictions = []\n",
        "all_bio_labels = []\n",
        "all_sentiment_predictions = []\n",
        "all_sentiment_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:  #dataloader del set di validazione/test\n",
        "        input_ids_batch, attention_masks_batch, bio_labels_batch, sentiment_labels_batch = batch\n",
        "\n",
        "        #apertura del file ndjson per il training\n",
        "        input_ids_batch = input_ids_batch.to(device)\n",
        "        attention_masks_batch = attention_masks_batch.to(device)\n",
        "        bio_labels_batch = bio_labels_batch.to(device)\n",
        "        sentiment_labels_batch = sentiment_labels_batch.to(device)\n",
        "\n",
        "        outputs = model(input_ids_batch, attention_mask=attention_masks_batch)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        bio_logits = logits[..., :3]\n",
        "        sentiment_logits = logits[..., 3:]\n",
        "\n",
        "        bio_predictions = torch.argmax(bio_logits, dim=-1)\n",
        "        sentiment_predictions = torch.argmax(sentiment_logits, dim=-1)\n",
        "\n",
        "        all_bio_predictions.extend(bio_predictions.view(-1).cpu().numpy())\n",
        "        all_bio_labels.extend(bio_labels_batch.view(-1).cpu().numpy())\n",
        "        all_sentiment_predictions.extend(sentiment_predictions.view(-1).cpu().numpy())\n",
        "        all_sentiment_labels.extend(sentiment_labels_batch.view(-1).cpu().numpy())\n",
        "\n",
        "#calcolo metriche di valutazione per BIO e sentiment\n",
        "bio_classification_report = classification_report(\n",
        "    all_bio_labels, all_bio_predictions, labels=list(bio_label_map.values()), target_names=list(bio_label_map.keys())\n",
        ")\n",
        "sentiment_classification_report = classification_report(\n",
        "    all_sentiment_labels, all_sentiment_predictions, labels=[0, 1, 2, 3], target_names=[\"positive\", \"negative\", \"neutral\", \"mixed\"], zero_division = 0.0\n",
        ")\n",
        "\n",
        "print(\"BIO Classification Report:\")\n",
        "print(bio_classification_report)\n",
        "\n",
        "print(\"\\nSentiment Classification Report:\")\n",
        "print(sentiment_classification_report)"
      ],
      "metadata": {
        "id": "Mk-Cj6fKZOoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Caricamento delle recensioni da Google Drive**"
      ],
      "metadata": {
        "id": "7NO1oeDUMiaj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "6SiRk2lyZRir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nltk\n",
        "\n",
        "#scarica il modello di divisione in frasi di NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "#funzione di tokenizzazione con offsets\n",
        "def tokenize_with_offsets(sentence, tokenizer):\n",
        "    return tokenizer(\n",
        "        sentence,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=32,\n",
        "        return_offsets_mapping=True\n",
        "    )\n",
        "\n",
        "#funzione per leggere il file, dividerlo in frasi e passare ogni frase alla funzione di tokenizzazione\n",
        "def process_text_file(file_path, tokenizer):\n",
        "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "        text = file.read()\n",
        "\n",
        "    #divide il testo in frasi usando NLTK\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "\n",
        "    #passa ogni frase alla funzione di tokenizzazione\n",
        "    tokenized_sentences = []\n",
        "    for sentence in sentences:\n",
        "        tokenized_sentence = tokenize_with_offsets(sentence, tokenizer)\n",
        "        tokenized_sentences.append({\n",
        "            \"sentence\": sentence,\n",
        "            \"tokenized\": tokenized_sentence\n",
        "        })\n",
        "\n",
        "    return tokenized_sentences\n",
        "\n",
        "\n",
        "folder_path = \"/content/drive/My Drive/recensioni/\"\n",
        "\n",
        "#dizionario per memorizzare i dati tokenizzati per ogni recensione\n",
        "tokenized_reviews = {}\n",
        "\n",
        "#scorre tutti i file nella cartella\n",
        "for file_name in os.listdir(folder_path):\n",
        "    file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "    #controlla che il file sia un file di testo\n",
        "    if file_name.endswith(\".txt\"):\n",
        "        print(f\"Processando il file: {file_name}\")\n",
        "\n",
        "        tokenized_data = process_text_file(file_path, tokenizer)\n",
        "\n",
        "        #salva i dati nel dizionario con il nome del file come chiave\n",
        "        tokenized_reviews[file_name] = tokenized_data"
      ],
      "metadata": {
        "id": "F9jakgs7ZTfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Funzioni per analisi delle recensioni**"
      ],
      "metadata": {
        "id": "hh2D3D2CODRb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#funzione per decodificare le predizioni\n",
        "id2label = {0: \"B-ASPECT\", 1: \"I-ASPECT\", 2: \"O\", 3:\"positive\", 4:\"negative\", 5: \"neutral\", 6:\"mixed\"}  #dizionario di mapping ID -> Etichetta stringa\n",
        "\n",
        "#funzione che ottiene le predizioni del modello sotto forma di logits e le mappa in etichette stringa\n",
        "def get_predictions(sentence, tokenized, model, tokenizer, id2label):\n",
        "\n",
        "    #evita problemi tra cpu e gpu, prendendo il device su cui Ã¨ il modello\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        #sposta i tensori sulla GPU se disponibile\n",
        "        inputs = {k: v.to(device) for k, v in tokenized.items() if k in [\"input_ids\", \"attention_mask\"]}\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "        #salva i logits\n",
        "        logits = outputs.logits.squeeze(0)  #rimuove solo la dimensione batch se presente\n",
        "\n",
        "        #BIO: primi 3 valori\n",
        "        bio_logits = logits[:, :3]\n",
        "        bio_predictions = torch.argmax(bio_logits, dim=-1).tolist()\n",
        "\n",
        "        #sentiment: ultimi 4 valori\n",
        "        sentiment_logits = logits[:, 3:]\n",
        "        sentiment_predictions = torch.argmax(sentiment_logits, dim=-1).tolist()\n",
        "\n",
        "    #converte gli ID in token\n",
        "    tokens = tokenizer.convert_ids_to_tokens(tokenized[\"input_ids\"].squeeze(0).tolist())\n",
        "\n",
        "    #converte le predizioni in etichette leggibili\n",
        "    bio_labels = [id2label[pred] for pred in bio_predictions]\n",
        "    sentiment_labels = [id2label[pred + 3] for pred in sentiment_predictions]  #aggiungo 3 per il mapping corretto\n",
        "\n",
        "    offsets = tokenized[\"offset_mapping\"].squeeze(0).tolist()\n",
        "\n",
        "    return tokens, bio_labels, sentiment_labels, offsets"
      ],
      "metadata": {
        "id": "JaZ-nXVoZZ8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#funzione che estrae gli aspetti a partire dalle predictions\n",
        "def extract_aspects(sentence, tokens, bio_labels, sentiment_labels, offsets):\n",
        "    aspects = []\n",
        "    current_aspect = []\n",
        "    current_sentiment = None\n",
        "    current_start = None\n",
        "\n",
        "    for token, bio_label, sentiment_label, (start, end) in zip(tokens, bio_labels, sentiment_labels, offsets):\n",
        "\n",
        "        if start == 0 and end == 0:  #ignora token di padding\n",
        "            continue\n",
        "\n",
        "        #controlla se il token Ã¨ una subword (inizia con ##, default di BERT)\n",
        "        is_subword = token.startswith(\"##\")\n",
        "\n",
        "\n",
        "        if bio_label == \"B-ASPECT\":\n",
        "\n",
        "            if current_aspect:  #salva l'aspetto con il rispettivo sentiment se Ã¨ definito\n",
        "                aspects.append((\"\".join(current_aspect).replace(\" ##\", \"\"), current_sentiment))\n",
        "\n",
        "            current_aspect = [token]\n",
        "            current_sentiment = sentiment_label\n",
        "            current_start = end\n",
        "\n",
        "        elif (bio_label == \"I-ASPECT\" or token.startswith(\"##\")) and current_aspect:\n",
        "            if is_subword:\n",
        "                current_aspect[-1] += token[2:]  #unisce la subword alla parola precedente (rimuove ## e unisce)\n",
        "            else:\n",
        "                current_aspect.append(\" \" + token)  #mantiene lo spazio tra parole separate\n",
        "\n",
        "            current_start = end\n",
        "\n",
        "        #se l'etichetta Ã¨ O o cambia tipo, salva e resetta\n",
        "        else:\n",
        "            if current_aspect:\n",
        "                aspects.append((\"\".join(current_aspect).replace(\" ##\", \"\"), current_sentiment))\n",
        "                current_aspect = []\n",
        "                current_sentiment = None\n",
        "                current_start = None\n",
        "\n",
        "    #aggiunge l'ultimo aspetto rilevato per non perderlo\n",
        "    if current_aspect:\n",
        "        aspects.append((\"\".join(current_aspect).replace(\" ##\", \"\"), current_sentiment))\n",
        "\n",
        "    return aspects"
      ],
      "metadata": {
        "id": "FTWL_yPlZc56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re #modulo per le espressioni regolari\n",
        "\n",
        "#funzione che estrae l'anno dal nome del file\n",
        "def extract_year_from_filename(filename):\n",
        "\n",
        "    match = re.search(r'\\d{4}', filename)\n",
        "    if match:\n",
        "        return int(match.group(0))  #restituisce l'anno come intero\n",
        "\n",
        "    return None  #se non viene trovato un anno, restituisce None\n",
        "\n",
        "#funzione per contare gli aspetti positivi e negativi\n",
        "def count_aspects(aspects):\n",
        "    positive_count = 0\n",
        "    negative_count = 0\n",
        "\n",
        "    for aspect in aspects:\n",
        "        sentiment = aspect[1]  #ottiene il sentiment dell'aspetto\n",
        "        try: #evita errori legati agli aspetti\n",
        "            if sentiment == \"positive\":\n",
        "                positive_count += 1\n",
        "            elif sentiment == \"negative\":\n",
        "                negative_count += 1\n",
        "        except ValueError:\n",
        "            print(f\"Formato sconosciuto dell'aspetto: {aspect}\")\n",
        "\n",
        "    return positive_count, negative_count\n",
        "\n",
        "\n",
        "review_summary = []\n",
        "\n",
        "counter = 0\n",
        "\n",
        "#iterazione sulle recensioni\n",
        "for review_name, tokenized_data in tokenized_reviews.items():\n",
        "    print(f\"Recensione: {review_name}\")\n",
        "\n",
        "    total_positive = 0\n",
        "    total_negative = 0\n",
        "    aspects_in_review = []\n",
        "\n",
        "    #estrae l'anno dal nome del file\n",
        "    year = extract_year_from_filename(review_name)\n",
        "\n",
        "    for entry in tokenized_data:  #itera sulle frasi della recensione\n",
        "        sentence = entry[\"sentence\"]\n",
        "        tokenized = entry[\"tokenized\"]\n",
        "\n",
        "        #genera predizioni ed estrae aspetti\n",
        "        tokens, bio_labels, sentiment_labels, offsets = get_predictions(sentence, tokenized, model, tokenizer, id2label)\n",
        "        aspects = extract_aspects(sentence, tokens, bio_labels, sentiment_labels, offsets)\n",
        "\n",
        "\n",
        "        #aggiunge gli aspetti a quelli globali\n",
        "        aspects_in_review.extend(aspects)\n",
        "\n",
        "        #conta gli aspetti positivi e negativi\n",
        "        positive_count, negative_count = count_aspects(aspects)\n",
        "        total_positive += positive_count\n",
        "        total_negative += negative_count\n",
        "\n",
        "    #aggiunge al summary delle recensioni\n",
        "    review_summary.append({\n",
        "          \"id\": counter, #id numerico\n",
        "          \"review_name\": review_name, #nome file\n",
        "          \"aspects\":aspects_in_review, #aspetti estratti\n",
        "          \"total_positive\": total_positive, #totale positivi\n",
        "          \"total_negative\": total_negative, #totale negativi\n",
        "          \"overall_sentiment\": \"positive\" if total_positive > total_negative else \"negative\" if total_negative > total_positive else \"neutral\", #sentiment complessivo sulla base della quantitÃ  di aspetti positivi o negativi estratti\n",
        "          \"year\": year  #anno\n",
        "    })\n",
        "\n",
        "    counter += 1"
      ],
      "metadata": {
        "id": "kJ_eO1mgZfpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Funzioni per la generazione dei grafici e delle tabelle**"
      ],
      "metadata": {
        "id": "1ADKLDADSNFj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tabella riassuntiva di tutte le recensioni**"
      ],
      "metadata": {
        "id": "GGvEYHFFUuSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(20, 20))\n",
        "\n",
        "# Aggiungi la tabella\n",
        "tabella = ax.table(\n",
        "    cellText=[[review[\"id\"], review[\"review_name\"], review[\"year\"], review[\"total_positive\"], review[\"total_negative\"]] for review in review_summary],\n",
        "    colLabels=[\"Id\", \"Nome\", \"Anno\", \"Asp. positivi\", \"Asp. negativi\"],\n",
        "    loc='center'\n",
        ")\n",
        "\n",
        "#formattazione\n",
        "ax.axis('off')\n",
        "\n",
        "tabella.auto_set_font_size(False)\n",
        "tabella.set_fontsize(15)\n",
        "tabella.scale(2, 2)\n",
        "\n",
        "\n",
        "for i, col in enumerate(tabella.get_celld().keys()):\n",
        "    row, col_index = col\n",
        "    if col_index != 1:\n",
        "        col_width = max(len(str(cell.get_text().get_text())) for cell in tabella.get_celld().values() if cell.get_text() is not None and cell.get_text().get_text())\n",
        "        tabella.auto_set_column_width([col_index])\n",
        "\n",
        "\n",
        "for i, key in enumerate(tabella.get_celld().keys()):\n",
        "    row, col = key\n",
        "    tabella.get_celld()[key].set_height(0.02)\n",
        "    tabella.get_celld()[key].set_text_props(ha='center', va='center')\n",
        "\n",
        "#salva immagine\n",
        "plt.savefig(\"recensioni.png\")\n",
        "\n",
        "#mostra tabella\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hOQ3Kkj2_e4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Grafico a dispersione per il sentiment generale delle recensioni**"
      ],
      "metadata": {
        "id": "KMq9p-e7VH4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from adjustText import adjust_text\n",
        "\n",
        "\n",
        "positive_aspects = [review['total_positive'] for review in review_summary]\n",
        "negative_aspects = [review['total_negative'] for review in review_summary]\n",
        "\n",
        "texts = []\n",
        "#crea grafico a dispersione\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.scatter(positive_aspects, negative_aspects, color='blue', label='Id recensione')\n",
        "\n",
        "for i in range(len(review_summary)):\n",
        "  texts.append(plt.text(positive_aspects[i], negative_aspects[i], f\"({review_summary[i]['id']})\", fontsize=10, ha='right', va='bottom', color='black'))\n",
        "adjust_text(texts)\n",
        "\n",
        "x = np.linspace(0, min(max(positive_aspects), max(negative_aspects)), 100)\n",
        "y = x\n",
        "\n",
        "plt.plot(x, y, color='red', linestyle='--')  #bisettrice\n",
        "\n",
        "plt.xticks(positive_aspects, labels=[str(aspect) for aspect in positive_aspects])\n",
        "plt.yticks(negative_aspects, labels=[str(aspect) for aspect in negative_aspects])\n",
        "\n",
        "\n",
        "plt.xlabel(\"# Aspetti positivi\")\n",
        "plt.ylabel(\"# Aspetti negativi\")\n",
        "plt.title(\"Aspetti positivi vs. negativi per recensione\")\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.savefig(\"Sentiment_complessivo.png\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Dekrs-lFZi1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Grafico per l'evoluzione del sentiment negli anni 1827-1841**"
      ],
      "metadata": {
        "id": "y6jCAruVVQjt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "review_summary = sorted(review_summary, key=lambda x: x['year'])  #ordina le recensioni per anno\n",
        "\n",
        "\n",
        "\n",
        "data = []\n",
        "flag = False\n",
        "\n",
        "\n",
        "for review in review_summary:\n",
        "    if review['year'] > 1841:\n",
        "        flag = True\n",
        "        break\n",
        "    data.append({\n",
        "        \"Anno\": review[\"year\"],\n",
        "        \"Sentiment\": review[\"overall_sentiment\"]\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "\n",
        "df_counts = df.groupby([\"Anno\", \"Sentiment\"]).size().unstack(fill_value=0)\n",
        "\n",
        "\n",
        "df_counts = df_counts.reset_index()\n",
        "\n",
        "\n",
        "sns.set_style(\"white\")\n",
        "plt.figure(figsize=(14, 7))\n",
        "\n",
        "\n",
        "ax = df_counts.set_index(\"Anno\").plot(kind=\"bar\", stacked=True, figsize=(14, 7),\n",
        "                                      color={\"positive\": \"green\", \"negative\": \"red\", \"neutral\": \"blue\"}, alpha=0.7)\n",
        "\n",
        "\n",
        "for p in ax.patches:\n",
        "    height = p.get_height()\n",
        "    if height > 0:\n",
        "        ax.annotate(f\"{int(height)}\", (p.get_x() + p.get_width() / 2, p.get_y() + height / 2),\n",
        "                    ha=\"center\", va=\"center\", fontsize=10, color=\"white\", fontweight=\"bold\")\n",
        "\n",
        "\n",
        "plt.xlabel(\"Anno\", fontsize=14, fontweight=\"bold\")\n",
        "plt.ylabel(\"# Recensioni\", fontsize=14, fontweight=\"bold\")\n",
        "plt.title(\"Sentiment Complessivi per Anno\", fontsize=16, fontweight=\"bold\")\n",
        "plt.xticks(rotation=45, fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "\n",
        "\n",
        "plt.legend(title=\"Sentiment\", fontsize=12)\n",
        "\n",
        "\n",
        "plt.savefig(\"Sentiment_per_Anno_Seaborn_Stacked.png\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "g3fF_SSYZmMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tabella degli aspetti piÃ¹ frequenti dell'edizione Ventisettana, Quarantana e a livello globale**"
      ],
      "metadata": {
        "id": "4ApHF4rqVeso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "#funzioni che permettono di estrarre i cinque aspetti piÃ¹ frequenti sfruttando hmap che mappa ogni aspetto nel numero di occorrenze\n",
        "\n",
        "#a partire da hmap estrae i cinque aspetti piÃ¹ frequenti (il numero puÃ² essere modificato)\n",
        "def five_aspects(hmap, num):\n",
        "  best = [((\"\", \"\"), 0) for i in range(num)]\n",
        "  for aspect in hmap:\n",
        "    for i in range(num):\n",
        "      if hmap[aspect] > best[i][1]:\n",
        "        best.insert(i, (aspect, hmap[aspect]))\n",
        "        best.pop()\n",
        "        break\n",
        "  return best\n",
        "\n",
        "#verifica se l'anno delle recensioni Ã¨ nel range e crea hmap\n",
        "def get_aspects(min_year, max_year, num):\n",
        "  hmap = defaultdict(int)\n",
        "  for review in review_summary:\n",
        "    if review[\"year\"] < max_year and review[\"year\"] >= min_year:\n",
        "      for aspect in review[\"aspects\"]:\n",
        "        hmap[aspect] += 1\n",
        "  return five_aspects(hmap, num)\n",
        "\n",
        "ventisettana = get_aspects(1827, 1840, 5)\n",
        "quarantana = get_aspects(1840, 1842, 5)\n",
        "globale = get_aspects(1827, 1953, 5)\n",
        "\n",
        "\n",
        "ventisettana = [ventisettana[i][0][0] + \" - \" + ventisettana[i][0][1] + \" - \" + f\"{ventisettana[i][1]}\" for i in range(len(ventisettana))]\n",
        "quarantana = [quarantana[i][0][0] + \" - \" + quarantana[i][0][1] + \" - \" + f\"{quarantana[i][1]}\" for i in range(len(quarantana))]\n",
        "globale = [globale[i][0][0] + \" - \" + globale[i][0][1] + \" - \" + f\"{globale[i][1]}\" for i in range(len(globale))]\n",
        "\n",
        "\n",
        "rows = [[f\"{i+1}Â°\", ventisettana[i], quarantana[i], globale[i]] for i in range(5)]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "#aggiunge la tabella\n",
        "tabella = ax.table(cellText=rows, colLabels=[\"\", \"Ventisettana\", \"Quarantana\", \"Globale\"], loc='center')\n",
        "\n",
        "#formattazione\n",
        "ax.axis('off')\n",
        "\n",
        "\n",
        "tabella.auto_set_font_size(False)\n",
        "tabella.set_fontsize(12)\n",
        "tabella.scale(1.2, 1.2)\n",
        "\n",
        "\n",
        "\n",
        "for i, col in enumerate(tabella.get_celld().keys()):\n",
        "    if col[0] == 0:\n",
        "        col_width = max(len(str(cell.get_text().get_text())) for cell in tabella.get_celld().values() if cell.get_text() is not None and cell.get_text().get_text())\n",
        "        tabella.auto_set_column_width(col_width)\n",
        "\n",
        "for i, key in enumerate(tabella.get_celld().keys()):\n",
        "    row, col = key\n",
        "\n",
        "    tabella.get_celld()[key].set_height(0.08)\n",
        "    tabella.get_celld()[key].set_text_props(ha='center', va='center')\n",
        "plt.savefig(\"chart.png\")\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "Vt_2A7ma2BL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tabella con valori medi per confronto delle recensioni della Ventisettana e della Quarantana**"
      ],
      "metadata": {
        "id": "nrlbDx-NWVk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#funzione che calcola il valore medio per ogni sentiment come totale delle recensioni con quel sentiment/totale recensioni dell'edizione\n",
        "def mean_sent(sentiment):\n",
        "    v_num, v_den, q_num, q_den = 0, 0, 0, 0\n",
        "    for review in review_summary:\n",
        "        if review[\"year\"] < 1840:\n",
        "            v_den += 1\n",
        "            v_num += (1 if review[\"overall_sentiment\"] == sentiment else 0)\n",
        "        elif review[\"year\"] < 1842:\n",
        "            q_den += 1\n",
        "            q_num += (1 if review[\"overall_sentiment\"] == sentiment else 0)\n",
        "    return v_num / v_den, q_num / q_den\n",
        "\n",
        "venti_p, quara_p = mean_sent(\"positive\")\n",
        "venti_neg, quara_neg = mean_sent(\"negative\")\n",
        "venti_neu, quara_neu = mean_sent(\"neutral\")\n",
        "\n",
        "venti = [\"Ventisettana\", venti_neu, venti_p, venti_neg]\n",
        "quara = [\"Quarantana\", quara_neu, quara_p, quara_neg]\n",
        "\n",
        "values = [venti, quara]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "tabella = ax.table(cellText=[[round(value[i], 3) if isinstance(value[i], float) else value[i] for i in range(len(value))] for value in values], colLabels=[\"Edizione\", \"Media rec. neutre\", \"Media asp. positive\", \"Media asp. negative\"], loc='center')\n",
        "\n",
        "#formattazione\n",
        "ax.axis('off')\n",
        "\n",
        "\n",
        "tabella.auto_set_font_size(False)\n",
        "tabella.set_fontsize(12)\n",
        "tabella.scale(1.2, 1.2)\n",
        "\n",
        "\n",
        "\n",
        "for i, col in enumerate(tabella.get_celld().keys()):\n",
        "    if col[0] == 0:\n",
        "        col_width = max(len(str(cell.get_text().get_text())) for cell in tabella.get_celld().values() if cell.get_text() is not None and cell.get_text().get_text())\n",
        "        tabella.auto_set_column_width(col_width)\n",
        "\n",
        "for i, key in enumerate(tabella.get_celld().keys()):\n",
        "    row, col = key\n",
        "\n",
        "    tabella.get_celld()[key].set_height(0.08)\n",
        "    tabella.get_celld()[key].set_text_props(ha='center', va='center')\n",
        "plt.savefig(\"media.png\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "SPkdSnIICU7k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}